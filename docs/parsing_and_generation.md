# Log Parsing and Code/Notebook Generation in `semTParser`

This document provides a comprehensive overview of how the `semTParser` project parses logs and generates Python scripts or Jupyter notebooks for semT table operations. It covers the flow from reading logs to producing executable code, detailing the logic, data structures, and design decisions.

---

## Table of Contents

1. [Overview](#overview)
2. [Log Parsing](#log-parsing)
    - [Log File Structure](#log-file-structure)
    - [Finding Relevant Log Segments](#finding-relevant-log-segments)
    - [Operation Extraction and Preprocessing](#operation-extraction-and-preprocessing)
    - [Operation Sorting and Filtering](#operation-sorting-and-filtering)
3. [Code and Notebook Generation](#code-and-notebook-generation)
    - [Python Script Generation](#python-script-generation)
    - [Jupyter Notebook Generation](#jupyter-notebook-generation)
    - [Base Code Templates](#base-code-templates)
    - [Operation Cell/Block Generation](#operation-cellblock-generation)
4. [Supporting Utilities](#supporting-utilities)
5. [Design Considerations](#design-considerations)
6. [Extending the System](#extending-the-system)

---

## Overview

The `semTParser` project automates the process of converting semT operation logs into executable Python scripts or Jupyter notebooks. This enables reproducibility and transparency for data operations performed via the semT platform. The workflow consists of:

1. **Parsing logs** to extract relevant operations.
2. **Preprocessing and filtering** these operations.
3. **Generating code** (Python or notebook) that replays the operations.

---

## Log Parsing

### Log File Structure

The log file (default: `logs.txt`) contains lines representing various operations performed on tables in the semT system. Each line typically encodes an operation with fields separated by `-|` and key-value pairs separated by `:`. Example:

```
[2024-06-01T12:00:00Z] -| OpType:RECONCILIATION -| ColumnName:city -| Reconciler:Q123 -| AdditionalData:{"properties":"P31 P625"}
```

Special operations like `GET_TABLE` and `SAVE_TABLE` are used as markers to delimit relevant log segments.

### Finding Relevant Log Segments

The function `logs_from_last_get_table` (in `src/operations.rs`) is responsible for extracting the log segment of interest:

- **Reverse Scan:** The log file is read in reverse to find the most recent `GET_TABLE` entry.
- **Segment Extraction:** All lines after this `GET_TABLE` (up to the next `SAVE_TABLE` or end of file) are collected.
- **Purpose:** This isolates the operations performed since the last table retrieval, ensuring only the latest workflow is replayed.

### Operation Extraction and Preprocessing

The function `pre_process_operations` parses each extracted log line into a `HashMap<String, String>`:

- **Splitting:** Each line is split by `-|` into fields.
- **Key-Value Parsing:** Each field is split by `:` into key and value.
- **Special Handling:** The `AdditionalData` field may contain JSON, which is preserved as a string for later parsing.
- **Timestamp Extraction:** Timestamps are parsed and normalized to RFC3339 format.

This results in a vector of operation maps, each representing a single operation with all its parameters.

### Operation Sorting and Filtering

The function `process_operations` performs two main tasks:

1. **Sorting:** Operations are sorted by timestamp (most recent first).
2. **Filtering:**
    - **Deduplication:** For `RECONCILIATION` operations, only the latest per column is kept.
    - **Ordering:** `RECONCILIATION` operations are prioritized to appear before others.

This ensures that the generated code reflects the most recent and relevant sequence of operations.

---

## Code and Notebook Generation

Depending on the user's choice (`--format python` or `--format notebook`), the system generates either a Python script or a Jupyter notebook that reproduces the operations.

### Python Script Generation

#### Entry Point and Arguments

- The main entry point is the `create_python` function in `src/python_helpers.rs`.
- It receives a vector of parsed operations (`Vec<HashMap<String, String>>`) and an `Args` struct containing the path to the table file.

#### File Creation and Structure

- A new Python file is created with a timestamp-based name (e.g., `base_file_2024-06-01_12-00.py`).
- The file is opened for writing (created if it does not exist, or appended to if it does).

#### Base File Loader

- The first block written is the base loader code, generated by `get_base_file_loader_code()` from `src/code_helper/mod.rs`.
    - This includes all necessary imports, authentication setup, and utility class instantiations.
    - Environment variables (username, password, base URL) are injected at runtime using Rust's `std::env::var` or replaced with defaults. The API URL is computed as base URL + "/api".

#### Table Loader Block

- The next block loads the dataset into a pandas DataFrame.
- The function `write_table_loader` is called, which:
    - Determines if any columns need to be deleted (by checking for a `SAVE_TABLE` operation with a `DeletedCols` field).
    - Calls either `get_base_dataset_loader` or `get_base_dataset_loader_with_column_deletion` to generate the appropriate code.
    - The generated code includes logic to prompt for dataset ID and table name, load the CSV, and optionally drop columns.

#### Operation Blocks

- For each operation in the sorted and filtered list:
    - The operation type is matched (`RECONCILIATION`, `EXTENSION`, etc.).
    - For `RECONCILIATION`:
        - `create_reconciliation_operation` is called, which uses `get_base_reconciliation_operation` to generate code for reconciling a column.
        - The code block includes error handling, calls to the reconciliation manager, and output display.
    - For `EXTENSION`:
        - `create_extension_operation` is called, which uses `get_base_extension_operation` to generate code for extending a column.
        - Properties and additional parameters are parsed from the `AdditionalData` JSON field.
    - Each generated code block is appended to the Python file.

#### Data Structures and Flow

- Operations are always handled as `HashMap<String, String>`, allowing flexible access to operation parameters.
- The code templates are Rust string constants with placeholders, filled in using `.replace()` for each operation.
- The file is written sequentially: base loader → table loader → operation blocks.

#### Output

- The final Python file is a runnable script that replays the semT operations in order.
- The path to the generated file is returned.


### Jupyter Notebook Generation

#### Entry Point and Arguments

- The main entry point is the `create_notebook` function in `src/notebook_helpers.rs`.
- It receives a vector of parsed operations and an `Args` struct containing the table file path.

#### Notebook Data Structures

- The notebook is constructed as a Rust struct (`Notebook`) with:
    - `cells`: a vector of `Cell` enums (either `Code` or `Markdown`)
    - `metadata`: a struct for notebook-level metadata (currently empty)
    - `nbformat` and `nbformat_minor`: notebook format version numbers

- Each `Cell` contains:
    - A unique `id` (generated with `uuid`)
    - `metadata` (currently empty JSON)
    - `source`: a vector of strings, each representing a line of code or markdown
    - For code cells: `execution_count` and `outputs` (both optional/empty)

#### Notebook Construction Flow

1. **Initial Imports Cell**
    - The first cell is a code cell containing the base loader code (from `get_base_file_loader_code()`).
    - This sets up all imports, authentication, and utility objects.

2. **Data Loading Cell**
    - The next cell loads the dataset and optionally deletes columns.
    - If a `SAVE_TABLE` operation with `DeletedCols` is found, `get_base_dataset_loader_with_column_deletion` is used; otherwise, `get_base_dataset_loader`.
    - The code is split into lines and stored as the cell's `source`.

3. **Operation Cells**
    - For each operation:
        - A Markdown cell is added describing the operation (e.g., "## Reconciliation operation for column city by Q123").
        - A Code cell is added with the operation logic, generated using the same templates as for Python scripts.
        - For `RECONCILIATION`, the code is generated by `get_base_reconciliation_operation`.
        - For `EXTENSION`, the code is generated by `get_base_extension_operation`, with properties parsed from the `AdditionalData` JSON.
        - Unknown operation types are noted in Markdown cells for traceability.

#### Serialization and Output

- The notebook struct is serialized to pretty-printed JSON using `serde_json`.
- The resulting JSON is written to a file named `base_notebook_file_<timestamp>.ipynb`.
- The path to the generated notebook is returned.

#### Technical Notes

- All code templates are split into lines for the notebook's `source` field, ensuring proper formatting in Jupyter.
- The notebook format version is set to 4.5 for compatibility.
- The system is designed to be easily extensible for new cell types or metadata.


### Base Code Templates

- All code generation is driven by templates defined as Rust string constants in `src/code_helper/mod.rs`.
- Each template contains placeholders (e.g., `__DATASET_ID__`, `__COLUMN_NAME__`) that are replaced at runtime using `.replace()`.

#### Main Templates

- **BASE_FILE_CONTENT:** Sets up imports, authentication, and utility objects. Used as the first cell/block in both Python and notebook outputs.
- **BASE_DATASET_LOAD_DATAFRAME:** Loads a CSV into a pandas DataFrame, optionally deletes columns, and adds the table to the dataset.
- **BASE_RECONCILE_OPERATION:** Contains the logic for running a reconciliation operation, including error handling and result display.
- **BASE_EXTENSION_OPERATION:** Contains the logic for running an extension operation, including error handling and result display.

#### Template Usage

- The helper functions (`get_base_file_loader_code`, `get_base_dataset_loader`, etc.) fill in the templates with operation-specific parameters.
- For notebooks, the template output is split into lines for the cell `source`.
- For Python scripts, the template output is appended as a code block.

#### Example

A reconciliation operation cell in the notebook might be generated as:

```python
reconciliator_id = "Q123"
optional_columns = []
column_name = "city"
try:
    table_data = table_manager.get_table(dataset_id, table_id)
    reconciled_table, backend_payload = reconciliation_manager.reconcile(
        table_data,
        column_name,
        reconciliator_id,
        optional_columns
    )
    # ...
except Exception as e:
    print(f"An error occurred during reconciliation: {e}")
```


### Operation Cell/Block Generation

#### Operation Handling Logic

- The main loop over operations (in both `create_python` and `create_notebook`) matches on the `OpType` field of each operation map.
- For each supported operation type:
    - **RECONCILIATION:**
        - Extracts `Reconciler` and `ColumnName` from the operation map.
        - Calls the reconciliation template generator with these parameters.
        - In notebooks, adds a Markdown cell describing the operation, then a Code cell with the generated code.
        - In Python scripts, appends the code block directly.
    - **EXTENSION:**
        - Extracts `Extender`, `ColumnName`, and parses `properties` from the `AdditionalData` JSON.
        - Calls the extension template generator with these parameters.
        - In notebooks, adds a Markdown cell and a Code cell as above.
        - In Python scripts, appends the code block.
    - **Other Operations:**
        - Currently, unknown or unsupported operation types are added as Markdown cells (notebooks) or logged as comments (Python).

#### Data Flow

- All operation parameters are accessed via the `HashMap<String, String>` for flexibility.
- JSON fields (like `AdditionalData`) are parsed using `serde_json` for property extraction.
- The code generation is modular: each operation type has a dedicated template and generator function.

#### Example Notebook Cell Sequence

1. Markdown: "## Reconciliation operation for column city by Q123"
2. Code: (generated reconciliation code)
3. Markdown: "## Extension operation for column country by Q456"
4. Code: (generated extension code)

---


## Supporting Utilities

- **JSON Parsing:** `parse_json` safely parses JSON strings from the `AdditionalData` field.
- **Deleted Columns:** `parse_deleted_columns` extracts column names from the `DeletedCols` field for column deletion logic.
- **File Handling:** Helper functions ensure files are created or appended to as needed.

---

## Design Considerations

- **Idempotency:** Only the latest relevant operations are replayed, avoiding duplication.
- **Extensibility:** New operation types can be added by extending the code templates and operation handling logic.
- **Separation of Concerns:** Parsing, preprocessing, and code generation are modularized for maintainability.
- **Environment Variables:** Authentication and base URL are injected via environment variables or user prompts for flexibility. The API URL is computed as base URL + "/api".

---

## Extending the System

To support new operation types or change the code generation logic:

1. **Add a Template:** Define a new code template in `src/code_helper/mod.rs`.
2. **Update Operation Handling:** Extend the match logic in `create_python` and `create_notebook` to handle the new operation type.
3. **Update Parsing (if needed):** Adjust `pre_process_operations` if the log format changes.

---

## Summary

The `semTParser` project provides a robust pipeline for transforming semT operation logs into reproducible code artifacts. By carefully parsing, filtering, and templating, it ensures that users can easily regenerate their data workflows in Python or Jupyter notebook form.

For further details, refer to the source code in the `src/` directory and the code templates in `src/code_helper/mod.rs`.